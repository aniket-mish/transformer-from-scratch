# transformers-from-scratch

Implementing transformers from scratch for a deeper understanding of the concepts involved.

## History

### Recurrent Neural Networks (RNN)

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

Disadvantages of RNNs:
- Cannot capture long term dependencies
- Computationally expensive
- Vanishing/exploding gradient problem
- Cannot process inputs in parallel

![image](https://github.com/aniket-mish/transformers-from-scratch/assets/71699313/65bc8013-dd6e-4d81-9bf8-f813ace2d977)

## Transformers

[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

## Input Embedding
Converts the input tokens into vectors.

## Positional Encoding
Tells the model about the position of a word in the sentence. It is computed once and then reused during training and inference.

## Add & Norm

## FeedForward

## Multi-Head Attention