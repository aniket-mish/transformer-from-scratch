# transformers-from-scratch
Implementing transformers from scratch for a deeper understanding of the concept

![image](https://github.com/aniket-mish/transformers-from-scratch/assets/71699313/65bc8013-dd6e-4d81-9bf8-f813ace2d977)


1. Input Embedding
Converts the input tokens into vectors.
2. Positional Encoding
Tells the model about the position of a word in the sentence. It is computed once and then reused during training and inference.
3. Add & Norm
