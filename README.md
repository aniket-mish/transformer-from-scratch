# transformers-from-scratch
Implementing transformers from scratch for deeper understanding of the concept


![Transformers Architecture](https://www.google.com/url?sa=i&url=https%3A%2F%2Ftowardsdatascience.com%2Fattention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634&psig=AOvVaw0w6ID4_kBqu6EPg-D50SUy&ust=1698407556561000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCNi9qq7Tk4IDFQAAAAAdAAAAABAD)


1. Input Embeddings
Converts the input tokens into vectors of dimension.
2. Positional Encoding
Tells model about the position of a word in the sentence. It is computed once and then reused during training and inference.

![Positional Encodings](https://www.google.com/url?sa=i&url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F68477306%2Fpositional-encoding-for-time-series-based-data-for-transformer-dnn-models&psig=AOvVaw3D6xjMmsx4bD3O53HnO_pa&ust=1698408290377000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCPDW7YvWk4IDFQAAAAAdAAAAABAP)