# transformers-from-scratch
Implementing transformers from scratch for a deeper understanding of the concept

![image](https://github.com/aniket-mish/transformers-from-scratch/assets/71699313/2048264f-f057-474c-a34b-92785009eba8)


1. Input Embedding
Converts the input tokens into vectors of dimension.
2. Positional Encoding
Tells the model about the position of a word in the sentence. It is computed once and then reused during training and inference.
3. Add & Norm